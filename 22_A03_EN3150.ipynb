{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQsKr65kX4Td"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EN3150 Assignment 03 – Task 1\n",
        "# ============================================================\n",
        "\n",
        "# We did the tasks as two divided parts; so, to see the outputs of these codes, please check our Github repo.\n",
        "# Task 01: https://github.com/Neuro-Matrix-EN3150/Assignment-03-CNN-for-image-classification/blob/main/CNN.ipynb\n",
        "# Task 02: https://github.com/Neuro-Matrix-EN3150/Assignment-03-CNN-for-image-classification/blob/main/Part2%20new.ipynb\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "import random, os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "MoP2xiVoYHM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Combine training and test sets\n",
        "X = np.concatenate([x_train, x_test], axis=0)\n",
        "y = np.concatenate([y_train, y_test], axis=0)\n",
        "\n",
        "# Normalize and reshape\n",
        "X = X.astype(\"float32\") / 255.0\n",
        "X = np.expand_dims(X, -1)\n",
        "\n",
        "# Split 70/15/15\n",
        "train_size = 0.70\n",
        "val_size = 0.15 / (1 - train_size)\n",
        "\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=train_size, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, train_size=val_size, stratify=y_rem, random_state=42)\n",
        "\n",
        "# One-hot encode\n",
        "num_classes = 10\n",
        "y_train_cat = to_categorical(y_train, num_classes)\n",
        "y_val_cat = to_categorical(y_val, num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes)\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Val:  \", X_val.shape)\n",
        "print(\"Test: \", X_test.shape)"
      ],
      "metadata": {
        "id": "GRpb2-ACYKwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape=(28,28,1), num_classes=10):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3,3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "        MaxPooling2D((2,2)),\n",
        "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2,2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "0CqAMuiJYNs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_lr = 1e-3\n",
        "opt = Adam(learning_rate=initial_lr)\n",
        "\n",
        "model = build_model()\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train_cat,\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_val, y_val_cat),\n",
        "    callbacks=[reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss (Adam)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kqYrRCA_YQy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_optimizer(opt, opt_name, epochs=20):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = build_model()\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    hist = model.fit(X_train, y_train_cat, validation_data=(X_val, y_val_cat),\n",
        "                     epochs=epochs, batch_size=128, verbose=2)\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    return {'name': opt_name, 'model': model, 'history': hist, 'test_acc': test_acc, 'y_pred': y_pred}\n",
        "\n",
        "results = []\n",
        "results.append(train_with_optimizer(Adam(learning_rate=1e-3), \"Adam\"))\n",
        "results.append(train_with_optimizer(SGD(learning_rate=0.01), \"SGD\"))\n",
        "results.append(train_with_optimizer(SGD(learning_rate=0.01, momentum=0.9), \"SGD+Momentum\"))\n",
        "\n",
        "for r in results:\n",
        "    print(f\"{r['name']} Test Accuracy: {r['test_acc']:.4f}\")"
      ],
      "metadata": {
        "id": "9E5DDV96YUf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in results:\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Optimizer: {r['name']}\")\n",
        "    print(\"Test Accuracy:\", round(r['test_acc'], 4))\n",
        "    print(classification_report(y_test, r['y_pred'], digits=4))\n",
        "\n",
        "    cm = confusion_matrix(y_test, r['y_pred'])\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix - {r[\"name\"]}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gSgfURs3YYY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best = results[0]  # e.g., Adam performed best\n",
        "y_pred = best['y_pred']\n",
        "\n",
        "print(\"Final Model Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision (macro):\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall (macro):\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1-score (macro):\", f1_score(y_test, y_pred, average='macro'))"
      ],
      "metadata": {
        "id": "PL2mmjR4a-ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EN3150 Assignment 03 – Task 2: Compare with State-of-the-Art Networks\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50, VGG16\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# ============================================================\n",
        "# 1. Load MNIST (UCI equivalent)\n",
        "# ============================================================\n",
        "(x_train_full, y_train_full), (x_test_full, y_test_full) = tf.keras.datasets.mnist.load_data()\n",
        "X_full = np.concatenate([x_train_full, x_test_full], axis=0)\n",
        "y_full = np.concatenate([y_train_full, y_test_full], axis=0)\n",
        "\n",
        "# Optional: use subset for Colab memory safety\n",
        "subset_size = 15000       # use 10k–20k to fit RAM\n",
        "X_full = X_full[:subset_size]\n",
        "y_full = y_full[:subset_size]\n",
        "\n",
        "# ============================================================\n",
        "# 2. Split dataset 70/15/15\n",
        "# ============================================================\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X_full, y_full, test_size=0.30, random_state=42, stratify=y_full\n",
        "    )\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 3. Preprocessing on-the-fly (memory efficient)\n",
        "# ============================================================\n",
        "IMG_SIZE = 224\n",
        "BATCH = 64\n",
        "\n",
        "def preprocess_grayscale_to_rgb(x, y):\n",
        "    x = tf.expand_dims(x, -1)                  # (28,28,1)\n",
        "    x = tf.image.resize(x, [IMG_SIZE, IMG_SIZE])\n",
        "    x = tf.image.grayscale_to_rgb(x)           # (224,224,3)\n",
        "    x = tf.cast(x, tf.float32) / 255.0\n",
        "    return x, y\n",
        "\n",
        "train_ds = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "            .shuffle(10000)\n",
        "            .map(preprocess_grayscale_to_rgb, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .batch(BATCH)\n",
        "            .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "val_ds = (tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "          .map(preprocess_grayscale_to_rgb, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "          .batch(BATCH)\n",
        "          .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "test_ds = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "           .map(preprocess_grayscale_to_rgb, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "           .batch(BATCH)\n",
        "           .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "print(\"Train:\", len(X_train), \"Val:\", len(X_val), \"Test:\", len(X_test))"
      ],
      "metadata": {
        "id": "TxMqMZCWbFcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4. Custom CNN Model\n",
        "# ============================================================\n",
        "def build_custom_cnn(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=10,\n",
        "                     x1=32, x2=64, x3=256, d=0.5):\n",
        "    inp = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(x1, (3,3), activation='relu', padding='same')(inp)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "    x = layers.Conv2D(x2, (3,3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "    x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(x3, activation='relu')(x)\n",
        "    x = layers.Dropout(d)(x)\n",
        "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    return models.Model(inp, out)\n",
        "\n",
        "custom_model = build_custom_cnn()\n",
        "custom_model.compile(optimizer=tf.keras.optimizers.Adam(3e-4),\n",
        "                     loss='sparse_categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "custom_model.summary()\n",
        "\n",
        "EPOCHS = 20\n",
        "history_custom = custom_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n",
        "\n",
        "# ============================================================\n",
        "# 5. Prepare datasets for ResNet and VGG (preprocessing)\n",
        "# ============================================================\n",
        "def map_resnet(x, y):\n",
        "    return resnet_preprocess(x * 255.0), y\n",
        "\n",
        "def map_vgg(x, y):\n",
        "    return vgg_preprocess(x * 255.0), y\n",
        "\n",
        "train_ds_resnet = train_ds.map(map_resnet).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds_resnet   = val_ds.map(map_resnet).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds_resnet  = test_ds.map(map_resnet).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds_vgg = train_ds.map(map_vgg).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds_vgg   = val_ds.map(map_vgg).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds_vgg  = test_ds.map(map_vgg).prefetch(tf.data.AUTOTUNE)\n",
        "\n"
      ],
      "metadata": {
        "id": "p_slCm3fbXhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6. ResNet50 Transfer Learning\n",
        "# ============================================================\n",
        "num_classes = 10\n",
        "base_res = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "base_res.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = base_res(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "resnet_model = models.Model(inputs, outputs)\n",
        "\n",
        "resnet_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                     loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_resnet_head = resnet_model.fit(train_ds_resnet, validation_data=val_ds_resnet, epochs=8)\n",
        "\n",
        "# Fine-tune\n",
        "base_res.trainable = True\n",
        "resnet_model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "                     loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_resnet_ft = resnet_model.fit(train_ds_resnet, validation_data=val_ds_resnet, epochs=12)"
      ],
      "metadata": {
        "id": "5gN7E0-cbiIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7. VGG16 Transfer Learning\n",
        "# ============================================================\n",
        "def map_resnet(x, y):\n",
        "    return resnet_preprocess(x * 255.0), y\n",
        "\n",
        "def map_vgg(x, y):\n",
        "    return vgg_preprocess(x * 255.0), y\n",
        "\n",
        "train_ds_resnet = train_ds.map(map_resnet).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds_resnet   = val_ds.map(map_resnet).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds_resnet  = test_ds.map(map_resnet).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds_vgg = train_ds.map(map_vgg).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds_vgg   = val_ds.map(map_vgg).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds_vgg  = test_ds.map(map_vgg).prefetch(tf.data.AUTOTUNE)\n",
        "num_classes = 10\n",
        "base_vgg = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "base_vgg.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = base_vgg(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "vgg_model = models.Model(inputs, outputs)\n",
        "\n",
        "vgg_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_vgg_head = vgg_model.fit(train_ds_vgg, validation_data=val_ds_vgg, epochs=8)\n",
        "\n",
        "base_vgg.trainable = True\n",
        "vgg_model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_vgg_ft = vgg_model.fit(train_ds_vgg, validation_data=val_ds_vgg, epochs=12)\n"
      ],
      "metadata": {
        "id": "eM0uopmjbmnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8. Plot training/validation losses\n",
        "# ============================================================\n",
        "\n",
        "def plot_history(histories, titles):\n",
        "    plt.figure(figsize=(12,5))\n",
        "    for h, t in zip(histories, titles):\n",
        "        plt.plot(h.history['loss'], label=f'{t} Train Loss')\n",
        "        plt.plot(h.history['val_loss'], '--', label=f'{t} Val Loss')\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "    plt.legend(); plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "# Note: history_custom is from the custom CNN model training\n",
        "# history_resnet_head and history_resnet_ft are from ResNet50 training\n",
        "# history_vgg_head and history_vgg_ft are from VGG16 training\n",
        "\n",
        "# Plot losses for Custom CNN\n",
        "if 'history_custom' in locals() and history_custom is not None:\n",
        "    plot_history([history_custom], ['Custom CNN'])\n",
        "\n",
        "# Plot losses for ResNet50 (fine-tuned) if available\n",
        "if 'history_resnet_ft' in locals() and history_resnet_ft is not None:\n",
        "    plot_history([history_resnet_ft], ['ResNet Fine-Tune'])\n",
        "\n",
        "# Plot losses for VGG16 (fine-tuned) if available\n",
        "if 'history_vgg_ft' in locals() and history_vgg_ft is not None:\n",
        "    plot_history([history_vgg_ft], ['History VGG Fine-Tune'])"
      ],
      "metadata": {
        "id": "vdJiLF2LbsT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 9. Evaluate and Compare Models\n",
        "# ============================================================\n",
        "# Apply preprocessing to test_ds for the custom model evaluation\n",
        "# Ensure IMG_SIZE is 224 for custom model evaluation\n",
        "IMG_SIZE_EVAL = 224 # Use a different variable name for clarity\n",
        "\n",
        "# Explicitly resize X_test before creating the dataset\n",
        "X_test_resized = tf.image.resize(tf.expand_dims(X_test, -1), [IMG_SIZE_EVAL, IMG_SIZE_EVAL]).numpy()\n",
        "X_test_rgb = tf.image.grayscale_to_rgb(tf.constant(X_test_resized, dtype=tf.float32)).numpy() / 255.0\n",
        "\n",
        "# Add print statement to check the shape of X_test_rgb\n",
        "print(\"Shape of X_test_rgb before creating dataset:\", X_test_rgb.shape)\n",
        "\n",
        "test_ds_custom = (tf.data.Dataset.from_tensor_slices((X_test_rgb, y_test))\n",
        "           .batch(BATCH)\n",
        "           .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "\n",
        "# Add print statement to check the shape of the data from the dataset\n",
        "for images, labels in test_ds_custom.take(1):\n",
        "    print(\"Shape of a batch from test_ds_custom:\", images.shape)\n",
        "\n",
        "test_loss_c, test_acc_c = custom_model.evaluate(test_ds_custom)\n",
        "print(\"Custom CNN Test accuracy:\", test_acc_c)\n",
        "\n",
        "test_loss_r, test_acc_r = resnet_model.evaluate(test_ds_resnet)\n",
        "print(\"ResNet50 Test accuracy:\", test_acc_r)\n",
        "\n",
        "test_loss_v, test_acc_v = vgg_model.evaluate(test_ds_vgg)\n",
        "print(\"VGG16 Test accuracy:\", test_acc_v)\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    'Model': ['Custom CNN', 'ResNet50', 'VGG16'],\n",
        "    'Test Accuracy': [test_acc_c, test_acc_r, test_acc_v]\n",
        "})\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "F1_gH7xFbvQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 10. Classification report & confusion matrix (optional)\n",
        "# ============================================================\n",
        "# Regenerate y_true based on the correctly preprocessed test_ds for the custom model\n",
        "y_true_custom = np.concatenate([y for _, y in test_ds_custom], axis=0)\n",
        "\n",
        "y_pred_c = np.argmax(custom_model.predict(test_ds_custom), axis=1)\n",
        "print(\"\\nCustom CNN:\\n\", classification_report(y_true_custom, y_pred_c, digits=4))\n",
        "cm_c = confusion_matrix(y_true_custom, y_pred_c)\n",
        "plt.figure(figsize=(8, 6)) # Added figure size for better visualization\n",
        "sns.heatmap(cm_c, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Custom CNN Confusion Matrix'); plt.show()\n",
        "\n",
        "# For ResNet50 evaluation\n",
        "y_true_resnet = np.concatenate([y for _, y in test_ds_resnet], axis=0)\n",
        "y_pred_r = np.argmax(resnet_model.predict(test_ds_resnet), axis=1)\n",
        "print(\"\\nResNet50:\\n\", classification_report(y_true_resnet, y_pred_r, digits=4))\n",
        "cm_r = confusion_matrix(y_true_resnet, y_pred_r)\n",
        "plt.figure(figsize=(8, 6)) # Added figure size for better visualization\n",
        "sns.heatmap(cm_r, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('ResNet50 Confusion Matrix'); plt.show()\n",
        "\n",
        "# For VGG16 evaluation\n",
        "y_true_vgg = np.concatenate([y for _, y in test_ds_vgg], axis=0)\n",
        "y_pred_v = np.argmax(vgg_model.predict(test_ds_vgg), axis=1)\n",
        "print(\"\\nVGG16:\\n\", classification_report(y_true_vgg, y_pred_v, digits=4))\n",
        "cm_v = confusion_matrix(y_true_vgg, y_pred_v)\n",
        "plt.figure(figsize=(8, 6)) # Added figure size for better visualization\n",
        "sns.heatmap(cm_v, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('VGG16 Confusion Matrix'); plt.show()"
      ],
      "metadata": {
        "id": "H8LBj5Hsb1FK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}